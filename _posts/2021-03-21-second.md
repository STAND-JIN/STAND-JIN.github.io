## 1. 벡터, 행렬 그리고 텐서(Vector, Matrix and Tensor)

### 1) 벡터, 행렬, 텐서 그림으로 이해하기

## 2. Numpy로 Tensor 만들기(벡터와 행렬 만들기)


```python
import numpy as np
```

### 1) 1D w/ Numpy


```python
t = np.array([0., 1., 2., 3., 4., 5., 6. ])
print(t)

print('Rank of t: ', t.ndim)
print('Shape of t: ', t.shape)
```

    [0. 1. 2. 3. 4. 5. 6.]
    Rank of t:  1
    Shape of t:  (7,)
    

#### 1-1) Numpy 기초 이해하기


```python
# 인덱스를 통한 원소 접근
print('t[0] t[1] t[-1] = ', t[0], t[1], t[-1])
```

    t[0] t[1] t[-1] =  0.0 1.0 6.0
    


```python
# [시작 번호 : 끝 번호]로 범위 지정을 통해 가져옴
# [: 끝 번호]는 끝 번호 생략
print('t[2:5] t[4:1] = ', t[2:5], t[4:-1])
```

    t[2:5] t[4:1] =  [2. 3. 4.] [4. 5.]
    


```python
# 시작 번호를 생략한 경우와 끝 번호를 생략한 경우
print('t[:2] t[:3] = ', t[:2], t[3:])
```

    t[:2] t[:3] =  [0. 1.] [3. 4. 5. 6.]
    

### 2) 2D w/ Numpy


```python
t = np.array([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.], [10., 11., 12.]])
print(t)
```

    [[ 1.  2.  3.]
     [ 4.  5.  6.]
     [ 7.  8.  9.]
     [10. 11. 12.]]
    


```python
print('Rank of t: ', t.ndim)
print('Shape of t: ', t.shape)
```

    Rank of t:  2
    Shape of t:  (4, 3)
    

## 3. 파이토치 텐서 선언하기(PyTorch Tensor Allocation)


```python
import torch
```

### 1) 1D w/ PyTorch


```python
t = torch.FloatTensor([0., 1., 2., 3., 4., 5., 6.])
print(t)
```

    tensor([0., 1., 2., 3., 4., 5., 6.])
    


```python
# rank == 차원
print(t.dim())

# shape
print(t.shape)
print(t.size())
```

    1
    torch.Size([7])
    torch.Size([7])
    


```python
# 인덱스로 접근
print(t[0], t[1], t[-1])
# 슬라이싱
print(t[2:5], t[4:-1])
# 슬라이싱
print(t[:2], t[3:])
```

    tensor(0.) tensor(1.) tensor(6.)
    tensor([2., 3., 4.]) tensor([4., 5.])
    tensor([0., 1.]) tensor([3., 4., 5., 6.])
    

### 2) 2D w/ PyTorch


```python
t = torch.FloatTensor([[1., 2., 3.],
                       [4., 5., 6.],
                       [7., 8., 9.],
                       [10., 11., 12.]
                       ])
print(t)
```

    tensor([[ 1.,  2.,  3.],
            [ 4.,  5.,  6.],
            [ 7.,  8.,  9.],
            [10., 11., 12.]])
    


```python
# rank == 차원
print(t.dim())
# shape
print(t.size())
```

    2
    torch.Size([4, 3])
    


```python
# 첫번째 차원을 전체 선택한 상황에서 두번째 차원의 첫번째 것만 가져옴
print(t[:, 1])
# 크기
print(t[:, 1].size())
```

    tensor([ 2.,  5.,  8., 11.])
    torch.Size([4])
    


```python
# 첫번째 차원을 전체 선택한 상황에서 마지막 차원의 첫번째 것만 가져옴
print(t[:, -1])
# 크기
print(t[:, -1].size())
```

    tensor([ 3.,  6.,  9., 12.])
    torch.Size([4])
    


```python
# 첫번째 차원을 전체 선택한 상황에서 두번째 차원에서는 맨 마지막에서 첫번째를 제외하고 다 가져옴
print(t[:, :-1])
```

    tensor([[ 1.,  2.],
            [ 4.,  5.],
            [ 7.,  8.],
            [10., 11.]])
    

## 3) 브로드캐스팅(Broadcasting)


```python
m1 = torch.FloatTensor([[3, 3]])
m2 = torch.FloatTensor([[2, 2]])
print(m1 + m2)
```

    tensor([[5., 5.]])
    


```python
# Vector + scalar
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([3])       # [3] -> [3, 3]
print(m1 + m2)
```

    tensor([[4., 5.]])
    


```python
# 2 X 1 Vector + 1 X 2 Vector
m1 = torch.FloatTensor([[1, 2]])
m2 = torch.FloatTensor([[3], [4]])
print(m1 + m2)   
```

    tensor([[4., 5.],
            [5., 6.]])
    

브로드캐스팅 과정에서 실제로 두 텐서가 어떻게 변경되는지 확인해보자.

```
[1, 2] ==> [[1, 2],
            [1, 2]]
```
```
[3]
[4]
==> [[3, 3],
     [4, 4]]
```

## 4) 자주 사용되는 기능들

### 1) 행렬 곱셈과 곱셈의 차이(Matrix Multiplication Vs. Multiplication

- 행렬로 곱셈을 하는 방법

    1) 행렬 곱셈(.matmul)

    2) 원소 별 곱셈(.mul)


```python
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape)   # 2 x 2
print('Shape of Matrix 2: ', m2.shape)   # 2 x 1
print(m1.matmul(m2))   # 2 x 1
```

    Shape of Matrix 1:  torch.Size([2, 2])
    Shape of Matrix 2:  torch.Size([2, 1])
    tensor([[ 5.],
            [11.]])
    

위의 결과는 2 x 2 행렬과 2 x 1 행렬(벡터)의 행렬 곱셈의 결과를 보여준다.

행렬 곱셈이 아닌 element-wise 곱셈이라는 것이 존재하는데, 이는 동일한 크기의 행렬이 동일한 위치에 있는 원소끼리 곱하는 것을 말한다. 이때는 * 또는 mul()을 사용한다.


```python
m1 = torch.FloatTensor([[1, 2], [3, 4]])
m2 = torch.FloatTensor([[1], [2]])
print('Shape of Matrix 1: ', m1.shape)   # 2 x 2
print('Shape of Matrix 2: ', m2.shape)   # 2 x 1
print(m1.matmul(m2))   # 2 x 1
```

    Shape of Matrix 1:  torch.Size([2, 2])
    Shape of Matrix 2:  torch.Size([2, 1])
    tensor([[ 5.],
            [11.]])
    


```python
print(m1 * m2)      # 2 x 2
print(m1.mul(m2))   # 2 x 2
```

    tensor([[1., 2.],
            [6., 8.]])
    tensor([[1., 2.],
            [6., 8.]])
    

브로드캐스팅 과정에서 m2 텐서가 어떻게 변경되는지 보자.

```
[1]
[2]
==> [[1, 1],
     [2, 2]]
```


```python

```
